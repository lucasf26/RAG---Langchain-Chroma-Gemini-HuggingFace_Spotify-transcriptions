{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG implementado con Langchain, base de datos vectorial Chroma, embeddings de HuggingFace y API de Gemini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se usa el dataset de transcripciones de podcast de Spotify.\n",
    "* Se cuenta también con un archivo de validación, que consiste en preguntas y las respuestas \"correctas\". Se usará estas respuestas para chequear que el RAG funciona bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referencias:\n",
    "* https://docs.mistral.ai/guides/rag/#rag-from-scratch\n",
    "* https://python.langchain.com/docs/integrations/vectorstores/chroma/\n",
    "* https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/miniconda3/envs/NLP/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import google.generativeai as genai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veo el archivo de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>human_answer</th>\n",
       "      <th>ai_answer_without_the_transcript</th>\n",
       "      <th>ai_answer_without_transcript_correctness</th>\n",
       "      <th>ai_answer_with_the_transcript</th>\n",
       "      <th>ai_answer_with_the_transcript_correctness</th>\n",
       "      <th>quality_rating_for_answer_with_transcript</th>\n",
       "      <th>post_url</th>\n",
       "      <th>file_name</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Airbnb go public, what was the price ...</td>\n",
       "      <td>December 9,2020 at $68 per share</td>\n",
       "      <td>Airbnb went public on December 10, 2020. The i...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Airbnb went public in 2020. However, the speci...</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>4</td>\n",
       "      <td>https://www.acquired.fm/episodes/airbnb</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did Wimdu unlike Airbnb not take off?</td>\n",
       "      <td>Wimdu gragmented the marketed focusing mostly ...</td>\n",
       "      <td>Wimdu faced challenges compared to Airbnb due ...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Wimdu, similar to Airbnb, was a platform creat...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.acquired.fm/episodes/airbnb</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why does market fragmentation work for airline...</td>\n",
       "      <td>Even though both the airline industry and airb...</td>\n",
       "      <td>Market fragmentation benefits the airline indu...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Market fragmentation can work for the airline ...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.acquired.fm/episodes/airbnb</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How many hot dogs does Costco currently sell p...</td>\n",
       "      <td>130 million</td>\n",
       "      <td>Costco sold just shy of 200 million hot dog an...</td>\n",
       "      <td>INCORRECT</td>\n",
       "      <td>Annual Hot Dog Sales: Costco sells 130 million...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.acquired.fm/episodes/costco</td>\n",
       "      <td>costco</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What store was created as \"the price club of h...</td>\n",
       "      <td>Home Depot</td>\n",
       "      <td>The store created as the \"price club of hardwa...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>Store Created as \"the price club of hardware s...</td>\n",
       "      <td>CORRECT</td>\n",
       "      <td>5</td>\n",
       "      <td>https://www.acquired.fm/episodes/costco</td>\n",
       "      <td>costco</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  When did Airbnb go public, what was the price ...   \n",
       "1          Why did Wimdu unlike Airbnb not take off?   \n",
       "2  Why does market fragmentation work for airline...   \n",
       "3  How many hot dogs does Costco currently sell p...   \n",
       "4  What store was created as \"the price club of h...   \n",
       "\n",
       "                                        human_answer  \\\n",
       "0                   December 9,2020 at $68 per share   \n",
       "1  Wimdu gragmented the marketed focusing mostly ...   \n",
       "2  Even though both the airline industry and airb...   \n",
       "3                                        130 million   \n",
       "4                                         Home Depot   \n",
       "\n",
       "                    ai_answer_without_the_transcript  \\\n",
       "0  Airbnb went public on December 10, 2020. The i...   \n",
       "1  Wimdu faced challenges compared to Airbnb due ...   \n",
       "2  Market fragmentation benefits the airline indu...   \n",
       "3  Costco sold just shy of 200 million hot dog an...   \n",
       "4  The store created as the \"price club of hardwa...   \n",
       "\n",
       "  ai_answer_without_transcript_correctness  \\\n",
       "0                                  CORRECT   \n",
       "1                                  CORRECT   \n",
       "2                                  CORRECT   \n",
       "3                                INCORRECT   \n",
       "4                                  CORRECT   \n",
       "\n",
       "                       ai_answer_with_the_transcript  \\\n",
       "0  Airbnb went public in 2020. However, the speci...   \n",
       "1  Wimdu, similar to Airbnb, was a platform creat...   \n",
       "2  Market fragmentation can work for the airline ...   \n",
       "3  Annual Hot Dog Sales: Costco sells 130 million...   \n",
       "4  Store Created as \"the price club of hardware s...   \n",
       "\n",
       "  ai_answer_with_the_transcript_correctness  \\\n",
       "0                                 INCORRECT   \n",
       "1                                   CORRECT   \n",
       "2                                   CORRECT   \n",
       "3                                   CORRECT   \n",
       "4                                   CORRECT   \n",
       "\n",
       "  quality_rating_for_answer_with_transcript  \\\n",
       "0                                         4   \n",
       "1                                         5   \n",
       "2                                         3   \n",
       "3                                         5   \n",
       "4                                         5   \n",
       "\n",
       "                                  post_url file_name Unnamed: 9  \n",
       "0  https://www.acquired.fm/episodes/airbnb    airbnb        NaN  \n",
       "1  https://www.acquired.fm/episodes/airbnb    airbnb        NaN  \n",
       "2  https://www.acquired.fm/episodes/airbnb    airbnb        NaN  \n",
       "3  https://www.acquired.fm/episodes/costco    costco        NaN  \n",
       "4  https://www.acquired.fm/episodes/costco    costco        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa = pd.read_csv('acquired-qa-evaluation.csv')\n",
    "df_qa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 80 entries, 0 to 79\n",
      "Data columns (total 10 columns):\n",
      " #   Column                                     Non-Null Count  Dtype \n",
      "---  ------                                     --------------  ----- \n",
      " 0   question                                   80 non-null     object\n",
      " 1   human_answer                               80 non-null     object\n",
      " 2   ai_answer_without_the_transcript           80 non-null     object\n",
      " 3   ai_answer_without_transcript_correctness   80 non-null     object\n",
      " 4   ai_answer_with_the_transcript              80 non-null     object\n",
      " 5   ai_answer_with_the_transcript_correctness  80 non-null     object\n",
      " 6   quality_rating_for_answer_with_transcript  80 non-null     object\n",
      " 7   post_url                                   80 non-null     object\n",
      " 8   file_name                                  80 non-null     object\n",
      " 9   Unnamed: 9                                 1 non-null      object\n",
      "dtypes: object(10)\n",
      "memory usage: 6.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_qa.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file_name\n",
       "qualcomm                                                6\n",
       "enron                                                   5\n",
       "spacex                                                  5\n",
       "bitcoin                                                 4\n",
       "airbnb                                                  3\n",
       "costco                                                  3\n",
       "disney_plus                                             3\n",
       "whatsapp                                                3\n",
       "berkshire_hathaway_part_i                               3\n",
       "nvidia_part_iii_the_dawn_of_the_ai_era_20222023         3\n",
       "nvidia_part_ii_the_machine_learning_company_20062022    3\n",
       "ethereum_with_packy_mccormick                           3\n",
       "nvidia_part_i_the_gpu_company_19932006                  3\n",
       "walmart                                                 3\n",
       "renaissance_technologies                                3\n",
       "nintendos_origins                                       3\n",
       "visa                                                    3\n",
       "amazoncom                                               3\n",
       "lvmh                                                    3\n",
       "doordash                                                2\n",
       "peloton                                                 2\n",
       "porsche_with_doug_demuro                                2\n",
       "arena_show_part_ii_brooks_running_with_ceo_jim_weber    2\n",
       "amazon_web_services                                     2\n",
       "the_electronic_arts_ipo_with_trip_hawkins               2\n",
       "https://www.acquired.fm/episodes/doordash               1\n",
       "the_nba                                                 1\n",
       "ftx_with_sam_bankmanfried_mario_gabriele                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qa.file_name.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son muchos archivos, así que me quedo solamente con los primeros 5 file_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ['qualcomm','enron','spacex','bitcoin','airbnb']\n",
    "df_qa.file_name.isin(file_names).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo un nuevo dataframe df_qf donde me quedo solamente con los datos correspondiente a estos 5 file_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>file_name</th>\n",
       "      <th>human_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When did Airbnb go public, what was the price ...</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>December 9,2020 at $68 per share</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why did Wimdu unlike Airbnb not take off?</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>Wimdu gragmented the marketed focusing mostly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why does market fragmentation work for airline...</td>\n",
       "      <td>airbnb</td>\n",
       "      <td>Even though both the airline industry and airb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>According to Information Theory, what is the i...</td>\n",
       "      <td>qualcomm</td>\n",
       "      <td>The more closely the actual communication is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Compare the impact on Qualcomm between the two...</td>\n",
       "      <td>qualcomm</td>\n",
       "      <td>Erwin Jacobs was a genius and visionary who pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question file_name  \\\n",
       "0   When did Airbnb go public, what was the price ...    airbnb   \n",
       "1           Why did Wimdu unlike Airbnb not take off?    airbnb   \n",
       "2   Why does market fragmentation work for airline...    airbnb   \n",
       "9   According to Information Theory, what is the i...  qualcomm   \n",
       "10  Compare the impact on Qualcomm between the two...  qualcomm   \n",
       "\n",
       "                                         human_answer  \n",
       "0                    December 9,2020 at $68 per share  \n",
       "1   Wimdu gragmented the marketed focusing mostly ...  \n",
       "2   Even though both the airline industry and airb...  \n",
       "9   The more closely the actual communication is t...  \n",
       "10  Erwin Jacobs was a genius and visionary who pa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_qf = df_qa[['question','file_name','human_answer']][df_qa.file_name.isin(file_names)]\n",
    "df_qf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tengo 5 archivos con 29 preguntas, con sus respectivas respuestas \"correctas\": respuesta humana 'human_answer' y los nombres de los archivos donde se encuentran las respuestas 'file_name'. Con estos dos datos voy a testear mi RAG. A partir de las preguntas testeo si el RAG: \n",
    "* busca la respuesta en el archivo correcto.\n",
    "* la distancia coseno entre la respuesta humana y la del LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ahora leo las transcripciones individuales y las spliteo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"../../acquired-individual-transcripts/\"\n",
    "\n",
    "docs = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    with open(PATH+file_name+'.txt','r') as file:\n",
    "        text = file.read()\n",
    "        doc =  Document(page_content=text[113:], metadata={\"source\": file_name})\n",
    "        docs.append(doc)\n",
    "\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 5 documents into 886 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"Split {len(docs)} documents into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'qualcomm'}, page_content=\"David: I walked in. The first thing I saw was the bottom of the big crane boom arm with the weights. I was like, why are there Olympic weights here? And then I was like, oh, because we got a professional boom arm camera. This is amazing. Ben: All right, let's do it. Welcome to season 11, episode 6 of Acquired, the podcast about great technology companies and the stories and playbooks behind them. I'm Ben Gilbert. I'm the Co-Founder and Managing Director of Seattle-based Pioneer Square Labs and our venture fund, PSL Ventures. David: I'm David Rosenthal. I'm an angel investor based in San Francisco. Ben: And we are your hosts. There's an incredible property of the universe where electromagnetic signals can be broadcast and travel through space at the speed of light to be received at a different point in the universe. Now, a tiny fraction of these frequencies are detectable by humans as visible light.  Some other frequencies can be dangerous, like X-rays or gamma rays. But there's a part\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo la base de datos con los chunks y embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargo los embeddings en local. modelo: multi-qa-MiniLM-L6-cos-v1\n",
    "model_name = \"/home/lucas/Documentos/Lucas/ML/embeddings/Sentence Transformers/\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name,model_kwargs=model_kwargs,encode_kwargs=encode_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo la base de datos Chroma con los documentos y embeddings\n",
    "#db = Chroma.from_documents(collection_name='chroma_lc_rag',documents=chunks, embedding=embeddings, persist_directory='chroma_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora puedo cargar la base de datos\n",
    "db = Chroma(collection_name='chroma_lc_rag',persist_directory=\"chroma_db\", embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is their charter, so he makes a $100,000 donation to The Mars Society. He joins the board and he starts meeting all of these aerospace people in LA. Not just in LA, of course back up in Silicon Valley there's NASA's Jet Propulsion Lab in Mountain View. \\xa0 Elon's mostly down in LA but he's going back and forth. He starts organizing these “Saturday salons,” he calls them, where he's just getting together industry leaders in aerospace and at JPL, both in LA and Palo Alto. There's no agenda, but he lets it be known to all of them that he's got some resources. He's a dot-com-rich guy and he wants to make a gesture. What could be done on the order of $10–$20 million.  They start to coalesce the group on this idea of building a “Mars Oasis” and the idea behind a Mars Oasis is that they're going to buy a rocket, and they're going to put a plant on it, and they're also going to put a robot on it, and they're going to shoot this rocket to mars. I can't remember if the Mars Rover had landed\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruebo que la base de datos funciona bien\n",
    "question = 'what is the idea about mars oasis'\n",
    "sim = db.similarity_search(question, k=1)\n",
    "sim[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métricas de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeo al hacer una pregunta, el chunk que devuelve Chroma esté en el documento correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23,)\n",
      "(23,)\n"
     ]
    }
   ],
   "source": [
    "response_file_name = []\n",
    "for question in df_qf.question:\n",
    "    sim = db.similarity_search(question, k=1)\n",
    "    response_file_name.append(sim[0].metadata['source'])\n",
    "\n",
    "response_file_name = pd.Series(response_file_name)\n",
    "print(response_file_name.shape)\n",
    "print(df_qf.file_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy = ',accuracy_score(response_file_name,df_qf.file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Osea que el 100% de los chunks devueltos por la base de datos, pertenecen al documento correcto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función para consultar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am a large language model, trained by Google.  I don't have a name or a personal identity. I'm an AI that processes information and responds to a wide range of prompts and questions.  I can provide summaries of factual topics, create stories, and translate languages.  I'm still under development, but I'm learning new things every day.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configuro la api de gemini\n",
    "# console: export GOOGLE_API_KEY=\"***api_key***\"\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "model.generate_content('hello! who are you?').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creo función para hacer consultas a la api de gemini\n",
    "def consulta(question):\n",
    "    sim = db.similarity_search(question, k=1)\n",
    "    retrieved_chunk = sim[0]\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Given the context information and not prior knowledge, answer the query.\n",
    "    Context information:\n",
    "    ---------------------\n",
    "    {retrieved_chunk.page_content}\n",
    "    ---------------------\n",
    "    Query: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "    \n",
    "    return retrieved_chunk, response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y la uso para hacer una nueva pregunta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:  what is the idea about mars oasis\n",
      "------\n",
      "response:  The idea behind Mars Oasis is to buy a rocket, put a plant and a robot on it, and send it to Mars.  The goal is to create a miniature oasis on the planet as a demonstrative gesture.\n",
      "\n",
      "chunk_source:  spacex\n"
     ]
    }
   ],
   "source": [
    "question_number=9\n",
    "question = df_qf['question'].iloc[question_number]\n",
    "print('question: ',question)\n",
    "print('------')\n",
    "chunk, response = consulta(question)\n",
    "print('response: ',response)\n",
    "print('chunk_source: ',chunk.metadata['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se cargaron los archivos a documentos de LangChain y se splitearon en 'chunks'.\n",
    "* Se creó la base de datos vectorial Chroma, donde se encuentra los chunks y los embeddings. El modelo de embeddings \"multi-qa-MiniLM-L6-cos-v1\" se obtuvo de HuggingFace.\n",
    "* Se testeó la performance de la base vectorial, obteniendo que apunta al archivo correctamente en el 100% de los casos.\n",
    "* Se implementó el RAG usando la base de datos vectorial Chroma y la API de Gemini.\n",
    "* Se probó el RAG con varias de las preguntas de validación y pareciera funcionar bien en general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabajo a futuro:\n",
    "* Faltaría hacer métricas de la performance del RAG, chequeando que las respuestas obtenidas sean las esperadas, usando las respuestas \"humanas\" del archivo de validación.\n",
    "* La selección de la longitud de los 'chunks' (chunk_size) es una variable importante que se puede ajustar en función de la performance obtenida de la base de datos vectorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-conda_env",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
